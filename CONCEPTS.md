# ğŸ§  Concepts DÃ©taillÃ©s - Comment Fonctionne le RAG

Guide visuel et pÃ©dagogique pour comprendre chaque Ã©tape du systÃ¨me.

---

## ğŸ“– Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Ã‰tape 1 : Extraction et Chunking](#Ã©tape-1--extraction-et-chunking)
3. [Ã‰tape 2 : Embeddings](#Ã©tape-2--embeddings)
4. [Ã‰tape 3 : Indexation FAISS](#Ã©tape-3--indexation-faiss)
5. [Ã‰tape 4 : Recherche](#Ã©tape-4--recherche)
6. [Ã‰tape 5 : GÃ©nÃ©ration](#Ã©tape-5--gÃ©nÃ©ration)
7. [Concepts avancÃ©s](#concepts-avancÃ©s)

---

## Vue d'ensemble

### Le problÃ¨me Ã  rÃ©soudre

**Sans RAG :**
```
Utilisateur: "Quel est l'article 5 du GDPR ?"
          â†“
    LLM (GPT-4)
          â†“
"Je pense que l'article 5 concerne..." â† HALLUCINATION possible!
```

**Avec RAG :**
```
Utilisateur: "Quel est l'article 5 du GDPR ?"
          â†“
     Recherche dans les documents
          â†“
    [Trouve l'article 5 exact]
          â†“
    LLM reÃ§oit le VRAI texte
          â†“
"L'article 5 du GDPR stipule..." â† FACTUEL et VÃ‰RIFIÃ‰!
```

### Architecture complÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1 : SETUP (une fois)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“„ PDFs                   ğŸ”ª Chunking              ğŸ”¢ Embeddings
â”Œâ”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GDPR â”‚                 â”‚ Chunk 1  â”‚            â”‚  Vector 1 â”‚
â”‚ 100  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚ Chunk 2  â”‚  â”€â”€â”€â”€â”€â”€â”€>  â”‚  Vector 2 â”‚
â”‚pages â”‚   extract_pdf   â”‚ Chunk 3  â”‚ embeddings â”‚  Vector 3 â”‚
â”‚      â”‚   .py           â”‚   ...    â”‚   .py      â”‚    ...    â”‚
â””â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â†“
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  FAISS Index     â”‚
                                              â”‚  (recherche      â”‚
                                              â”‚   ultra-rapide)  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 2 : REQUÃŠTE (Ã  chaque question)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â“ Question
   "What is GDPR?"
          â”‚
          â†“
   ğŸ” Vectorisation
   [0.1, 0.2, ...]
          â”‚
          â†“
   ğŸ“Š FAISS Search
   Trouve les 3 chunks
   les plus proches
          â”‚
          â†“
   ğŸ¤– LLM avec contexte
   GPT-4o-mini gÃ©nÃ¨re
   la rÃ©ponse
          â”‚
          â†“
   ğŸ’¬ RÃ©ponse + sources
```

---

## Ã‰tape 1 : Extraction et Chunking

### Pourquoi dÃ©couper ?

Un document de 100 pages = ~50,000 mots.
Un LLM ne peut pas tout traiter d'un coup !

**Solution : Le Chunking**

```
Document original (100 pages)
â”‚
â”œâ”€ Chunk 1  [mots 1-1000]     â† 1er morceau
â”œâ”€ Chunk 2  [mots 800-1800]   â† Overlap de 200 mots avec Chunk 1
â”œâ”€ Chunk 3  [mots 1600-2600]  â† Overlap de 200 mots avec Chunk 2
â””â”€ ...
```

### L'importance de l'overlap

**Sans overlap :**
```
Chunk 1: "...the data controller must"
Chunk 2: "ensure that personal data is..."

âŒ La phrase est coupÃ©e entre 2 chunks !
```

**Avec overlap :**
```
Chunk 1: "...the data controller must ensure..."
Chunk 2: "...controller must ensure that personal data is..."
                       â†‘
              200 mots de chevauchement
âœ… Contexte prÃ©servÃ© !
```

### ParamÃ¨tres optimaux

| ParamÃ¨tre | Valeur | Pourquoi |
|-----------|--------|----------|
| `chunk_size` | 1000 mots | Ã‰quilibre contexte/prÃ©cision |
| `overlap` | 200 mots | 20% de chevauchement |

**Trop petit (100 mots)** â†’ Perd le contexte
**Trop grand (5000 mots)** â†’ Pas assez prÃ©cis

---

## Ã‰tape 2 : Embeddings

### Qu'est-ce qu'un embedding ?

Un **embedding** transforme du texte en nombres (vecteur).

```python
Texte : "GDPR protects personal data"
         â†“
Embedding : [0.123, -0.456, 0.789, 0.234, ...]
            â† 1536 nombres (dimensions)
```

### La magie : SimilaritÃ© sÃ©mantique

Les textes similaires ont des vecteurs proches :

```
ğŸ“ Distance dans l'espace vectoriel

"data protection law"  â—
                        \  distance = 0.15 (proche!)
                         \
"privacy regulation"      â—

                            
                              . 
"chocolate recipe"            â—  
                              â†‘
                    distance = 0.95 (loin!)
```

### Visualisation (simplifiÃ©e Ã  2D)

En rÃ©alitÃ©, les embeddings ont 1536 dimensions, mais on peut les visualiser en 2D :

```
        Y
        â”‚
        â”‚     â— "GDPR"
        â”‚    â—â— "data protection"
        â”‚   â—â—â— "privacy law"
        â”‚  
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X
        â”‚
        â”‚                 â— "pizza"
        â”‚                 â— "recipe"
        â”‚
```

### Pourquoi `text-embedding-3-small` ?

| ModÃ¨le | Dimensions | CoÃ»t | QualitÃ© |
|--------|-----------|------|---------|
| text-embedding-3-small | 1536 | $0.02/1M tokens | â­â­â­â­ |
| text-embedding-3-large | 3072 | $0.13/1M tokens | â­â­â­â­â­ |
| text-embedding-ada-002 | 1536 | $0.10/1M tokens | â­â­â­ |

**Verdict :** `small` = parfait pour ce projet !

---

## Ã‰tape 3 : Indexation FAISS

### Le problÃ¨me de la recherche naÃ¯ve

**Sans index (recherche linÃ©aire) :**

```python
# Pour trouver les 3 chunks les plus proches
for chunk in all_chunks:  # 10,000 chunks
    distance = calculate_distance(query, chunk)
    
# Comparaisons nÃ©cessaires : 10,000
# Temps : 2-3 secondes â±ï¸ (trop lent!)
```

**Avec FAISS :**

```python
index.search(query_vector, k=3)

# Comparaisons : ~100-200 (approximation intelligente)
# Temps : 1-5 millisecondes âš¡ (300x plus rapide!)
```

### Comment FAISS accÃ©lÃ¨re la recherche

FAISS utilise des structures de donnÃ©es optimisÃ©es :

```
IndexFlatL2 (notre cas) : Recherche exacte
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tous les vecteurs en mÃ©moire      â”‚
â”‚  Calcul optimisÃ© avec SIMD/GPU     â”‚
â”‚  Parfait pour < 1M vecteurs        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IndexIVF (pour gros datasets) : Recherche approximative
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰tape 1: Clustering (groupes)     â”‚
â”‚    â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”              â”‚
â”‚    â”‚ G1â”‚ â”‚ G2â”‚ â”‚ G3â”‚              â”‚
â”‚    â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜              â”‚
â”‚  Ã‰tape 2: Cherche dans 1-2 groupes â”‚
â”‚  â†’ 10x plus rapide, ~95% prÃ©cis    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Type de distance

**L2 (Euclidienne)** : Distance "en ligne droite"

```
Point A: [1, 2]     Point B: [4, 6]

distance = âˆš[(4-1)Â² + (6-2)Â²]
         = âˆš[9 + 16]
         = 5
```

**Pourquoi L2 ?**
- Simple et efficace
- Fonctionne bien avec les embeddings OpenAI
- Standard de l'industrie

---

## Ã‰tape 4 : Recherche

### Le processus de recherche

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question: "What is personal data?"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  1. Vectorisation de la question â”‚
        â”‚     OpenAI embeddings             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
               [0.234, -0.123, 0.567, ...]
                       â”‚
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2. Recherche FAISS (k=3)        â”‚
        â”‚     Trouve les 3 vecteurs les    â”‚
        â”‚     plus proches                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  RÃ©sultats :                     â”‚
        â”‚  â€¢ Chunk 42 (distance: 0.15)     â”‚
        â”‚  â€¢ Chunk 18 (distance: 0.23)     â”‚
        â”‚  â€¢ Chunk 7  (distance: 0.31)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### InterprÃ©tation des distances

```
Distance    SimilaritÃ©    InterprÃ©tation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.00-0.20   TrÃ¨s haute    Exactement ce qu'on cherche
0.20-0.40   Haute         TrÃ¨s pertinent
0.40-0.60   Moyenne       Potentiellement utile
0.60-1.00   Faible        Peu pertinent
> 1.00      TrÃ¨s faible   Hors sujet
```

### ParamÃ¨tre k (nombre de chunks)

```
k=1  â† Trop restrictif, risque de manquer du contexte
k=3  â† Ã‰quilibre optimal (par dÃ©faut)
k=5  â† Plus de contexte, mais risque de bruit
k=10 â† Trop large, dilue l'information pertinente
```

**ExpÃ©rimentation :**
```python
# Question simple â†’ k=1 suffit
"What is GDPR?" â†’ k=1

# Question complexe â†’ k=5 recommandÃ©
"How do data controllers ensure GDPR compliance across different EU member states?" â†’ k=5
```

---

## Ã‰tape 5 : GÃ©nÃ©ration

### Construction du prompt

Le prompt est **crucial** pour la qualitÃ© de la rÃ©ponse :

```python
# âŒ Mauvais prompt
prompt = f"{context}\n\nQuestion: {query}"

# âœ… Bon prompt
prompt = f"""
You are a legal assistant expert.

IMPORTANT: Answer ONLY based on the context below.
If the answer is not in the context, say so clearly.

Context:
{context}

Question: {query}

Provide a clear answer with citations.
"""
```

### Anatomie d'un bon prompt

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RÃ”LE : "You are a legal assistant"         â”‚
â”‚     â†’ DÃ©finit le comportement                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. INSTRUCTION : "Answer ONLY based on..."    â”‚
â”‚     â†’ Ã‰vite les hallucinations                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. CONTEXTE : Les chunks rÃ©cupÃ©rÃ©s            â”‚
â”‚     â†’ Les donnÃ©es factuelles                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. QUESTION : La question utilisateur         â”‚
â”‚     â†’ Ce qu'on veut savoir                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. FORMAT : "Provide clear answer with..."    â”‚
â”‚     â†’ Structure de la rÃ©ponse                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ParamÃ¨tres du LLM

```python
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[...],
    temperature=0.3,     # â† Important!
    max_tokens=500,
    top_p=1.0
)
```

**Temperature** : ContrÃ´le la crÃ©ativitÃ©

```
temperature=0.0  â†’ DÃ©terministe, rÃ©pÃ©titif
                   "Personal data means..."
                   
temperature=0.3  â†’ LÃ©gÃ¨rement crÃ©atif (RAG optimal)
                   "Personal data refers to..."
                   
temperature=0.7  â†’ CrÃ©atif
                   "You know, personal data is like..."
                   
temperature=1.0  â†’ TrÃ¨s crÃ©atif, risque de dÃ©viation
                   "Imagine personal data as a treasure..."
```

**Pour le RAG : 0.2-0.4 est optimal** (factuel mais naturel)

---

## Concepts avancÃ©s

### 1. Metadata Filtering

**Actuel :**
```python
# Cherche dans TOUS les documents
results = retriever.search("GDPR principles")
```

**AmÃ©liorÃ© :**
```python
# Cherche UNIQUEMENT dans le GDPR
results = retriever.search(
    "GDPR principles",
    filters={"source": "GDPR.pdf"}
)
```

**ImplÃ©mentation :**
```python
chunk = {
    "text": "...",
    "source": "GDPR.pdf",
    "article": "Article 5",  # â† Metadata
    "chapter": "Chapter II",
    "page": 12
}
```

### 2. Reranking

AmÃ©liore la pertinence des rÃ©sultats :

```
FAISS (rapide, ~90% prÃ©cis)
         â†“
   Top 10 chunks
         â†“
Reranker (lent, 99% prÃ©cis)
         â†“
   Top 3 chunks
```

**ModÃ¨les de reranking :**
- Cohere Rerank API
- Cross-encoders (HuggingFace)

### 3. Hybrid Search

Combine recherche sÃ©mantique + recherche par mots-clÃ©s :

```
Question: "Article 5 GDPR"
         â”‚
         â”œâ”€â†’ Semantic search (FAISS)
         â”‚   â†’ Trouve passages sur "principles"
         â”‚
         â””â”€â†’ Keyword search (BM25)
             â†’ Trouve "Article 5" exactement
                      â†“
              Fusion des rÃ©sultats
```

### 4. Query Expansion

Enrichit la question :

```
Question originale:
"What is GDPR?"

Expansion automatique via LLM:
"What is GDPR? What is the General Data Protection Regulation? 
 What are the main objectives of GDPR?"

â†’ Recherche plus complÃ¨te!
```

### 5. Prompt Engineering AvancÃ©

**Few-shot learning :**
```python
prompt = f"""
Examples:
Q: What is Article 1?
A: Article 1 of GDPR states... [Source: GDPR.pdf, Article 1]

Q: Who is a controller?
A: According to Article 4... [Source: GDPR.pdf, Article 4]

Now answer this:
Q: {user_question}
A:
"""
```

---

## ğŸ“ RÃ©sumÃ© des concepts clÃ©s

| Concept | But | Technologie |
|---------|-----|-------------|
| **Chunking** | DÃ©couper les documents | PyMuPDF + Python |
| **Embeddings** | Vectoriser le texte | OpenAI API |
| **Indexation** | Recherche rapide | FAISS |
| **Retrieval** | Trouver les passages | FAISS search |
| **Generation** | CrÃ©er la rÃ©ponse | GPT-4o-mini |
| **RAG** | Combiner tout Ã§a | Architecture custom |

---

## ğŸ“š Pour aller plus loin

### Livres
- "Building LLM Applications" - O'Reilly
- "Natural Language Processing with Transformers"

### Cours
- DeepLearning.AI - "LangChain for LLM Application Development"
- Fast.AI - "Practical Deep Learning"

### Papers
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Facebook AI)
- "FAISS: A Library for Efficient Similarity Search" (Facebook AI)

---

**Maintenant tu comprends tous les concepts ! ğŸ‰**

Retourne au [README](README.md) ou au [QUICKSTART](QUICKSTART.md) pour commencer Ã  coder !

