"""
Module d'embeddings et d'indexation FAISS
==========================================

Ce module g√®re :
1. La cr√©ation d'embeddings vectoriels avec OpenAI
2. La construction de l'index FAISS pour la recherche rapide
3. La sauvegarde et le chargement de l'index

Qu'est-ce qu'un embedding ?
- C'est une repr√©sentation vectorielle (liste de nombres) d'un texte
- Les textes similaires ont des vecteurs proches
- Permet de faire de la recherche s√©mantique (par le sens, pas les mots exacts)

Qu'est-ce que FAISS ?
- Biblioth√®que Facebook pour la recherche de vecteurs similaires
- Ultra-rapide, m√™me avec des millions de vecteurs
- 100% local et gratuit !
"""

import os
import pickle
import numpy as np
import faiss
from openai import OpenAI
from typing import List, Dict
from dotenv import load_dotenv

# Charge les variables d'environnement depuis .env
load_dotenv()


class EmbeddingManager:
    """
    Classe pour g√©rer les embeddings OpenAI et l'indexation FAISS.
    """
    
    def __init__(self, api_key: str = None, model: str = "text-embedding-3-small"):
        """
        Initialise le gestionnaire d'embeddings.
        
        Args:
            api_key: Cl√© API OpenAI (si None, lit depuis .env)
            model: Mod√®le d'embedding √† utiliser
                   text-embedding-3-small : le moins cher (~$0.02/1M tokens)
        """
        # R√©cup√®re la cl√© API
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("‚ùå Cl√© API OpenAI manquante! Cr√©e un fichier .env avec OPENAI_API_KEY=...")
        
        # Initialise le client OpenAI
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        
        print(f"‚úÖ Client OpenAI initialis√© avec le mod√®le : {model}")
    
    def create_embedding(self, text: str) -> List[float]:
        """
        Cr√©e un embedding pour un texte donn√©.
        
        Args:
            text: Le texte √† vectoriser
            
        Returns:
            Liste de nombres repr√©sentant le vecteur
        """
        # Appel √† l'API OpenAI
        response = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        
        # Retourne le vecteur
        return response.data[0].embedding
    
    def create_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> np.ndarray:
        """
        Cr√©e des embeddings pour plusieurs textes en batch.
        
        Pourquoi en batch ?
        - Plus rapide que un par un
        - R√©duit le nombre d'appels API
        - √âconomise de l'argent
        
        Args:
            texts: Liste de textes √† vectoriser
            batch_size: Nombre de textes par batch
            
        Returns:
            Matrice numpy contenant tous les vecteurs
        """
        embeddings = []
        total = len(texts)
        
        print(f"\nüî¢ Cr√©ation de {total} embeddings par batch de {batch_size}...")
        
        # Traite par batch
        for i in range(0, total, batch_size):
            batch = texts[i:i + batch_size]
            
            # Appel API pour le batch
            response = self.client.embeddings.create(
                model=self.model,
                input=batch
            )
            
            # Extrait les vecteurs
            batch_embeddings = [item.embedding for item in response.data]
            embeddings.extend(batch_embeddings)
            
            # Affiche la progression
            progress = min(i + batch_size, total)
            print(f"   üìä Progression : {progress}/{total} ({100*progress//total}%)")
        
        # Convertit en numpy array (format attendu par FAISS)
        embeddings_array = np.array(embeddings).astype('float32')
        print(f"‚úÖ Embeddings cr√©√©s : shape = {embeddings_array.shape}\n")
        
        return embeddings_array
    
    def create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """
        Cr√©e un index FAISS pour la recherche rapide.
        
        Comment fonctionne FAISS ?
        - IndexFlatL2 : recherche exhaustive par distance euclidienne
        - Simple et pr√©cis (parfait pour < 1M de vecteurs)
        - Pour plus de vecteurs, on pourrait utiliser IndexIVF (clustering)
        
        Args:
            embeddings: Matrice des vecteurs (nombre_vecteurs x dimension)
            
        Returns:
            Index FAISS pr√™t √† l'emploi
        """
        # R√©cup√®re la dimension des vecteurs
        dimension = embeddings.shape[1]
        
        print(f"üèóÔ∏è  Cr√©ation de l'index FAISS...")
        print(f"   Dimension des vecteurs : {dimension}")
        print(f"   Nombre de vecteurs : {embeddings.shape[0]}")
        
        # Cr√©e l'index (IndexFlatL2 = recherche exacte par distance L2)
        index = faiss.IndexFlatL2(dimension)
        
        # Ajoute tous les vecteurs √† l'index
        index.add(embeddings)
        
        print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs\n")
        
        return index
    
    def save_index(self, index: faiss.Index, chunks: List[Dict], 
                   index_path: str = "index/legal.faiss", 
                   chunks_path: str = "index/chunks.pkl"):
        """
        Sauvegarde l'index FAISS et les chunks sur le disque.
        
        Pourquoi sauvegarder ?
        - √âvite de recr√©er les embeddings √† chaque fois (co√ªteux!)
        - Chargement instantan√© au d√©marrage de l'API
        
        Args:
            index: L'index FAISS √† sauvegarder
            chunks: Les chunks de texte correspondants
            index_path: Chemin de sauvegarde de l'index
            chunks_path: Chemin de sauvegarde des chunks
        """
        # Cr√©e le dossier si n√©cessaire
        os.makedirs(os.path.dirname(index_path), exist_ok=True)
        
        # Sauvegarde l'index FAISS
        faiss.write_index(index, index_path)
        print(f"üíæ Index FAISS sauvegard√© : {index_path}")
        
        # Sauvegarde les chunks avec pickle
        with open(chunks_path, 'wb') as f:
            pickle.dump(chunks, f)
        print(f"üíæ Chunks sauvegard√©s : {chunks_path}")
        
        # Calcule la taille
        index_size = os.path.getsize(index_path) / (1024 * 1024)  # En MB
        chunks_size = os.path.getsize(chunks_path) / (1024 * 1024)
        print(f"   üì¶ Taille totale : {index_size + chunks_size:.2f} MB\n")
    
    def load_index(self, index_path: str = "index/legal.faiss", 
                   chunks_path: str = "index/chunks.pkl"):
        """
        Charge l'index FAISS et les chunks depuis le disque.
        
        Args:
            index_path: Chemin de l'index FAISS
            chunks_path: Chemin des chunks
            
        Returns:
            Tuple (index FAISS, liste des chunks)
        """
        print(f"üìÇ Chargement de l'index existant...")
        
        # Charge l'index FAISS
        index = faiss.read_index(index_path)
        print(f"   ‚úÖ Index charg√© : {index.ntotal} vecteurs")
        
        # Charge les chunks
        with open(chunks_path, 'rb') as f:
            chunks = pickle.load(f)
        print(f"   ‚úÖ Chunks charg√©s : {len(chunks)} chunks\n")
        
        return index, chunks
    
    def index_exists(self, index_path: str = "index/legal.faiss", 
                     chunks_path: str = "index/chunks.pkl") -> bool:
        """
        V√©rifie si un index existe d√©j√†.
        
        Returns:
            True si l'index existe, False sinon
        """
        return os.path.exists(index_path) and os.path.exists(chunks_path)


# Exemple d'utilisation si ex√©cut√© directement
if __name__ == "__main__":
    from src.extract_pdf import PDFExtractor
    
    print("=== Test du module d'embeddings ===\n")
    
    # 1. Extrait les PDFs
    extractor = PDFExtractor()
    chunks = extractor.process_all_pdfs()
    
    if not chunks:
        print("‚ö†Ô∏è  Aucun chunk √† traiter. Ajoute des PDFs dans data/pdfs/")
        exit(1)
    
    # 2. Cr√©e les embeddings
    manager = EmbeddingManager()
    texts = [chunk["text"] for chunk in chunks]
    embeddings = manager.create_embeddings_batch(texts)
    
    # 3. Cr√©e l'index FAISS
    index = manager.create_faiss_index(embeddings)
    
    # 4. Sauvegarde
    manager.save_index(index, chunks)
    
    print("‚úÖ Indexation termin√©e avec succ√®s!")

