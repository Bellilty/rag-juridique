"""
Module d'extraction et de chunking des PDFs
============================================

Ce module g√®re :
1. L'extraction du texte depuis des fichiers PDF
2. Le nettoyage du texte (espaces, sauts de ligne, etc.)
3. Le d√©coupage (chunking) en morceaux optimaux pour la recherche

Pourquoi le chunking ?
- Les LLMs ont une limite de tokens
- Les petits morceaux permettent une recherche plus pr√©cise
- L'overlap √©vite de couper des phrases importantes
"""

import fitz  # PyMuPDF
import re
import os
from typing import List, Dict


class PDFExtractor:
    """
    Classe pour extraire et traiter le texte des PDFs juridiques.
    """
    
    def __init__(self, pdf_directory: str = "data/pdfs"):
        """
        Initialise l'extracteur de PDF.
        
        Args:
            pdf_directory: Chemin vers le dossier contenant les PDFs
        """
        self.pdf_directory = pdf_directory
        
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        Extrait tout le texte d'un fichier PDF.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Le texte complet extrait du PDF
        """
        print(f"üìÑ Extraction du PDF : {pdf_path}")
        
        # Ouvre le document PDF
        doc = fitz.open(pdf_path)
        text = ""
        
        # Parcourt chaque page
        for page_num, page in enumerate(doc, 1):
            # Extrait le texte de la page
            page_text = page.get_text("text")
            text += page_text
            
        doc.close()
        print(f"   ‚úÖ {len(text)} caract√®res extraits")
        return text
    
    def clean_text(self, text: str) -> str:
        """
        Nettoie le texte extrait :
        - Supprime les espaces multiples
        - Normalise les sauts de ligne
        - Supprime les caract√®res sp√©ciaux inutiles
        
        Args:
            text: Texte brut √† nettoyer
            
        Returns:
            Texte nettoy√©
        """
        # Remplace les multiples espaces par un seul
        text = re.sub(r'\s+', ' ', text)
        
        # Supprime les espaces en d√©but et fin
        text = text.strip()
        
        # Remplace les doubles sauts de ligne par un seul
        text = re.sub(r'\n\n+', '\n\n', text)
        
        return text
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict[str, any]]:
        """
        D√©coupe le texte en morceaux (chunks) avec chevauchement.
        
        Pourquoi le chevauchement (overlap) ?
        - √âvite de couper des informations importantes entre 2 chunks
        - Assure la continuit√© du contexte
        
        Args:
            text: Texte √† d√©couper
            chunk_size: Taille approximative de chaque chunk (en mots)
            overlap: Nombre de mots qui se chevauchent entre chunks
            
        Returns:
            Liste de dictionnaires contenant les chunks et leurs m√©tadonn√©es
        """
        # Divise le texte en mots
        words = text.split()
        chunks = []
        
        # Cr√©e les chunks avec chevauchement
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = " ".join(chunk_words)
            
            # Stocke le chunk avec ses m√©tadonn√©es
            chunks.append({
                "text": chunk_text,
                "chunk_id": len(chunks),
                "start_word": i,
                "end_word": min(i + chunk_size, len(words))
            })
        
        print(f"üî™ Texte d√©coup√© en {len(chunks)} chunks")
        print(f"   üìè Taille moyenne : {sum(len(c['text']) for c in chunks) // len(chunks)} caract√®res")
        
        return chunks
    
    def extract_text_from_txt(self, txt_path: str) -> str:
        """
        Extrait le texte d'un fichier TXT.
        
        Args:
            txt_path: Chemin vers le fichier TXT
            
        Returns:
            Le texte du fichier
        """
        print(f"üìÑ Extraction du TXT : {txt_path}")
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
            
        print(f"   ‚úÖ {len(text)} caract√®res extraits")
        return text
    
    def process_all_pdfs(self, chunk_size: int = 1000, overlap: int = 200) -> List[Dict[str, any]]:
        """
        Traite tous les PDFs et TXTs du dossier et retourne tous les chunks.
        
        Args:
            chunk_size: Taille des chunks en mots
            overlap: Chevauchement entre chunks
            
        Returns:
            Liste de tous les chunks de tous les fichiers
        """
        all_chunks = []
        
        # V√©rifie si le dossier existe
        if not os.path.exists(self.pdf_directory):
            print(f"‚ö†Ô∏è  Le dossier {self.pdf_directory} n'existe pas!")
            return all_chunks
        
        # R√©cup√®re tous les fichiers PDF et TXT
        pdf_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.pdf')]
        txt_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.txt')]
        all_files = pdf_files + txt_files
        
        if not all_files:
            print(f"‚ö†Ô∏è  Aucun fichier PDF/TXT trouv√© dans {self.pdf_directory}")
            return all_chunks
        
        print(f"\nüìö Traitement de {len(pdf_files)} PDF(s) et {len(txt_files)} TXT(s)...\n")
        
        # Traite chaque PDF
        for pdf_file in pdf_files:
            pdf_path = os.path.join(self.pdf_directory, pdf_file)
            
            # Extraction
            raw_text = self.extract_text_from_pdf(pdf_path)
            
            # Nettoyage
            clean_text = self.clean_text(raw_text)
            
            # Chunking
            chunks = self.chunk_text(clean_text, chunk_size, overlap)
            
            # Ajoute la source √† chaque chunk
            for chunk in chunks:
                chunk["source"] = pdf_file
            
            all_chunks.extend(chunks)
            print()
        
        # Traite chaque TXT
        for txt_file in txt_files:
            txt_path = os.path.join(self.pdf_directory, txt_file)
            
            # Extraction
            raw_text = self.extract_text_from_txt(txt_path)
            
            # Nettoyage
            clean_text = self.clean_text(raw_text)
            
            # Chunking
            chunks = self.chunk_text(clean_text, chunk_size, overlap)
            
            # Ajoute la source √† chaque chunk
            for chunk in chunks:
                chunk["source"] = txt_file
            
            all_chunks.extend(chunks)
            print()
        
        print(f"‚úÖ Total : {len(all_chunks)} chunks cr√©√©s depuis {len(all_files)} fichier(s)\n")
        
        return all_chunks


# Exemple d'utilisation si ex√©cut√© directement
if __name__ == "__main__":
    extractor = PDFExtractor()
    chunks = extractor.process_all_pdfs()
    
    # Affiche un exemple de chunk
    if chunks:
        print("üìã Exemple de chunk :")
        print(f"   Source : {chunks[0]['source']}")
        print(f"   ID : {chunks[0]['chunk_id']}")
        print(f"   Texte (100 premiers caract√®res) : {chunks[0]['text'][:100]}...")

