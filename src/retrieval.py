"""
Module de recherche et g√©n√©ration RAG
======================================

Ce module g√®re :
1. La recherche de chunks pertinents avec FAISS
2. La g√©n√©ration de r√©ponses avec OpenAI (RAG)
3. L'int√©gration du contexte r√©cup√©r√© dans le prompt

Qu'est-ce que le RAG (Retrieval-Augmented Generation) ?
- Retrieval : On cherche les passages pertinents dans nos documents
- Augmented : On enrichit le prompt du LLM avec ces passages
- Generation : Le LLM g√©n√®re une r√©ponse bas√©e sur ce contexte

Avantages du RAG :
- Le LLM r√©pond avec VOS donn√©es (pas ses connaissances g√©n√©rales)
- R√©duit les hallucinations
- Permet de citer les sources
"""

import numpy as np
import faiss
from openai import OpenAI
from typing import List, Dict, Tuple
import os
from dotenv import load_dotenv

load_dotenv()


class RAGRetriever:
    """
    Classe pour la recherche et la g√©n√©ration avec RAG.
    """
    
    def __init__(self, index: faiss.Index, chunks: List[Dict], api_key: str = None):
        """
        Initialise le retriever RAG.
        
        Args:
            index: Index FAISS charg√©
            chunks: Liste des chunks de texte
            api_key: Cl√© API OpenAI
        """
        self.index = index
        self.chunks = chunks
        
        # Initialise le client OpenAI
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("‚ùå Cl√© API OpenAI manquante!")
        
        self.client = OpenAI(api_key=api_key)
        
        print(f"‚úÖ RAG Retriever initialis√©")
        print(f"   üìö {len(chunks)} chunks disponibles")
        print(f"   üîç Index avec {index.ntotal} vecteurs\n")
    
    def create_query_embedding(self, query: str, model: str = "text-embedding-3-small") -> np.ndarray:
        """
        Cr√©e l'embedding de la question de l'utilisateur.
        
        Args:
            query: La question pos√©e
            model: Mod√®le d'embedding (doit √™tre le m√™me que pour l'index!)
            
        Returns:
            Vecteur numpy de la question
        """
        response = self.client.embeddings.create(
            model=model,
            input=query
        )
        
        # Convertit en numpy array
        embedding = np.array([response.data[0].embedding]).astype('float32')
        return embedding
    
    def search(self, query: str, k: int = 3) -> List[Dict]:
        """
        Recherche les k chunks les plus pertinents pour la question.
        
        Comment √ßa marche ?
        1. On vectorise la question
        2. FAISS trouve les k vecteurs les plus proches dans l'index
        3. On r√©cup√®re les chunks correspondants
        
        Args:
            query: La question de l'utilisateur
            k: Nombre de chunks √† retourner
            
        Returns:
            Liste des k chunks les plus pertinents avec leurs scores
        """
        print(f"üîç Recherche pour : '{query}'")
        
        # Cr√©e l'embedding de la question
        query_embedding = self.create_query_embedding(query)
        
        # Recherche dans FAISS
        # D = distances (plus petit = plus proche)
        # I = indices des chunks dans notre liste
        distances, indices = self.index.search(query_embedding, k)
        
        # Pr√©pare les r√©sultats
        results = []
        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
            chunk = self.chunks[idx].copy()
            chunk['distance'] = float(distance)
            chunk['rank'] = i + 1
            results.append(chunk)
            
            print(f"   {i+1}. [Distance: {distance:.2f}] {chunk['source']} - Chunk {chunk['chunk_id']}")
        
        print()
        return results
    
    def generate_answer(self, query: str, context_chunks: List[Dict], 
                       model: str = "gpt-4o-mini", max_tokens: int = 500) -> Dict:
        """
        G√©n√®re une r√©ponse en utilisant le RAG.
        
        Processus :
        1. On r√©cup√®re les chunks pertinents (d√©j√† fait avec search())
        2. On construit un prompt avec le contexte
        3. On demande au LLM de r√©pondre UNIQUEMENT bas√© sur ce contexte
        4. Le LLM g√©n√®re une r√©ponse avec citations
        
        Args:
            query: Question de l'utilisateur
            context_chunks: Chunks pertinents trouv√©s
            model: Mod√®le OpenAI √† utiliser
                   gpt-4o-mini : le meilleur rapport qualit√©/prix (~$0.15/1M tokens output)
                   gpt-3.5-turbo : encore moins cher mais moins bon
            max_tokens: Nombre max de tokens dans la r√©ponse
            
        Returns:
            Dictionnaire avec la r√©ponse et les m√©tadonn√©es
        """
        # Construit le contexte √† partir des chunks
        context = ""
        for i, chunk in enumerate(context_chunks, 1):
            context += f"[Extrait {i} - Source: {chunk['source']}]\n"
            context += chunk['text']
            context += "\n\n"
        
        # Construit le prompt pour le LLM
        # C'est ici qu'on "programme" le comportement du LLM
        system_prompt = """Tu es un assistant juridique expert.
R√©ponds UNIQUEMENT en te basant sur les extraits de documents fournis.
Si la r√©ponse n'est pas dans les extraits, dis clairement "Je ne trouve pas cette information dans les documents fournis."
Cite toujours la source (ex: [Source: GDPR.pdf]).
Sois pr√©cis et professionnel."""

        user_prompt = f"""Contexte (extraits de documents juridiques) :

{context}

Question : {query}

R√©ponds de mani√®re claire et cite tes sources."""

        print(f"ü§ñ G√©n√©ration de la r√©ponse avec {model}...")
        
        # Appel √† l'API OpenAI
        completion = self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=max_tokens,
            temperature=0.3  # Temp√©rature basse = r√©ponses plus d√©terministes et factuelles
        )
        
        # Extrait la r√©ponse
        answer = completion.choices[0].message.content
        
        # Pr√©pare le r√©sultat avec m√©tadonn√©es
        result = {
            "query": query,
            "answer": answer,
            "sources": [chunk['source'] for chunk in context_chunks],
            "num_chunks_used": len(context_chunks),
            "model": model,
            "tokens_used": {
                "prompt": completion.usage.prompt_tokens,
                "completion": completion.usage.completion_tokens,
                "total": completion.usage.total_tokens
            }
        }
        
        print(f"‚úÖ R√©ponse g√©n√©r√©e ({result['tokens_used']['total']} tokens utilis√©s)\n")
        
        return result
    
    def ask(self, query: str, k: int = 3, model: str = "gpt-4o-mini") -> Dict:
        """
        M√©thode principale : pose une question et obtient une r√©ponse RAG.
        
        C'est la m√©thode "tout-en-un" qui :
        1. Cherche les chunks pertinents
        2. G√©n√®re la r√©ponse
        
        Args:
            query: Question de l'utilisateur
            k: Nombre de chunks √† utiliser comme contexte
            model: Mod√®le OpenAI pour la g√©n√©ration
            
        Returns:
            Dictionnaire avec la r√©ponse compl√®te
        """
        # 1. Recherche les chunks pertinents
        relevant_chunks = self.search(query, k=k)
        
        # 2. G√©n√®re la r√©ponse
        result = self.generate_answer(query, relevant_chunks, model=model)
        
        return result


# Fonction utilitaire pour calculer le co√ªt approximatif
def estimate_cost(tokens_used: Dict, model: str = "gpt-4o-mini") -> Dict:
    """
    Estime le co√ªt d'une requ√™te RAG.
    
    Prix approximatifs (novembre 2024) :
    - gpt-4o-mini : $0.15/1M input tokens, $0.60/1M output tokens
    - gpt-3.5-turbo : $0.50/1M input, $1.50/1M output
    - text-embedding-3-small : $0.02/1M tokens
    
    Args:
        tokens_used: Dict avec 'prompt' et 'completion'
        model: Mod√®le utilis√©
        
    Returns:
        Dict avec le co√ªt estim√©
    """
    # Prix par 1M de tokens
    prices = {
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        "gpt-4": {"input": 30.00, "output": 60.00}
    }
    
    if model not in prices:
        model = "gpt-4o-mini"  # Par d√©faut
    
    # Calcule le co√ªt
    input_cost = (tokens_used["prompt"] / 1_000_000) * prices[model]["input"]
    output_cost = (tokens_used["completion"] / 1_000_000) * prices[model]["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_cost_usd": round(input_cost, 6),
        "output_cost_usd": round(output_cost, 6),
        "total_cost_usd": round(total_cost, 6),
        "model": model
    }


# Exemple d'utilisation si ex√©cut√© directement
if __name__ == "__main__":
    from embeddings import EmbeddingManager
    
    print("=== Test du module de retrieval ===\n")
    
    # Charge l'index existant
    manager = EmbeddingManager()
    
    if not manager.index_exists():
        print("‚ùå Aucun index trouv√©. Lance d'abord embeddings.py pour cr√©er l'index.")
        exit(1)
    
    index, chunks = manager.load_index()
    
    # Cr√©e le retriever
    retriever = RAGRetriever(index, chunks)
    
    # Pose une question test
    test_query = "What are the main principles of data protection?"
    result = retriever.ask(test_query, k=3)
    
    # Affiche les r√©sultats
    print("=" * 80)
    print(f"‚ùì Question : {result['query']}")
    print("=" * 80)
    print(f"\nüí¨ R√©ponse :\n{result['answer']}\n")
    print("=" * 80)
    print(f"üìö Sources : {', '.join(set(result['sources']))}")
    print(f"üî¢ Tokens utilis√©s : {result['tokens_used']['total']}")
    
    # Estime le co√ªt
    cost = estimate_cost(result['tokens_used'], result['model'])
    print(f"üí∞ Co√ªt estim√© : ${cost['total_cost_usd']} USD")
    print("=" * 80)

